{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation\n",
    "\n",
    "\n",
    "This is an example of applying Non-negative Matrix Factorization\n",
    "and Latent Dirichlet Allocation on a corpus of documents and\n",
    "extract additive models of the topic structure of the corpus.\n",
    "The output is a list of topics, each represented as a list of terms\n",
    "(weights are not shown).\n",
    "\n",
    "The default parameters (n_samples / n_features / n_topics) should make\n",
    "the example runnable in a couple of tens of seconds. You can try to\n",
    "increase the dimensions of the problem, but be aware that the time\n",
    "complexity is polynomial in NMF. In LDA, the time complexity is\n",
    "proportional to (n_samples * iterations).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading dataset from http://people.csail.mit.edu/jrennie/20Newsgroups/20news-bydate.tar.gz (14 MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "done in 85.016s.\n",
      "Extracting tf-idf features for NMF...\n",
      "done in 0.051s.\n",
      "Extracting tf features for LDA...\n",
      "done in 0.041s.\n",
      "Fitting the NMF model with tf-idf features, n_samples=200 and n_features=100...\n",
      "done in 0.089s.\n",
      "\n",
      "Topics in NMF model:\n",
      "Topic #0:\n",
      "just use like want long got way really bit make right years software work doesn new 10 used going cache\n",
      "Topic #1:\n",
      "think going sure computer year way got try did lot win com message look package called life software gm bit\n",
      "Topic #2:\n",
      "edu work good using mail problem new systems try little program drive site make use computer does look years format\n",
      "Topic #3:\n",
      "know people time say like does real help israel things want new mail life make information lot called program support\n",
      "Topic #4:\n",
      "don problem sure doesn hard years want way make like check did lot does based little time ll year say\n",
      "Topic #5:\n",
      "ve good year better win ll does years lot got support things life card 10 way using long com really\n",
      "Topic #6:\n",
      "key chip government public keys right phone real ll years going believe doesn gov line use used got help new\n",
      "Topic #7:\n",
      "car power believe used did look got bit sure problem good time know gov going win using new great try\n",
      "Topic #8:\n",
      "need really 16 hard runs drive know files com doesn try imagine little file check year sure message card government\n",
      "Topic #9:\n",
      "god believe way life did say does new things like doesn really make great time year years going better good\n",
      "\n",
      "Fitting LDA models with tf features, n_samples=200 and n_features=100...\n",
      "done in 0.268s.\n",
      "\n",
      "Topics in LDA model:\n",
      "Topic #0:\n",
      "key chip keys public government use used years work help doesn believe software 10 need phone good ll like right\n",
      "Topic #1:\n",
      "going card cache long info got hard used does good car people com public way government time really right little\n",
      "Topic #2:\n",
      "cache right new called great got like real think better support file card years runs does drive use check gov\n",
      "Topic #3:\n",
      "believe great god look new things bit using problem really people data just did used sure say need car 10\n",
      "Topic #4:\n",
      "israel real time people really government try just called problem public bit know information going great way sure little did\n",
      "Topic #5:\n",
      "edu graphics mail send 128 3d com file format objects image package files available tar stuff data help message based\n",
      "Topic #6:\n",
      "don just like think know people time use win way want lot years good sure new really work say did\n",
      "Topic #7:\n",
      "does god know good better ve life like year just don people say time make need way really doesn ll\n",
      "Topic #8:\n",
      "drive power program hard using problem does did good line work check need cache file use windows files new right\n",
      "Topic #9:\n",
      "gm edu graphics 24 mail site 3d send 16 new stuff objects 128 work years ll com phone files code\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Author: Olivier Grisel <olivier.grisel@ensta.org>\n",
    "#         Lars Buitinck\n",
    "#         Chyi-Kwei Yau <chyikwei.yau@gmail.com>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "from __future__ import print_function\n",
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "n_samples = 200\n",
    "n_features = 100\n",
    "n_topics = 10\n",
    "n_top_words = 20\n",
    "\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "\n",
    "\n",
    "# Load the 20 newsgroups dataset and vectorize it. We use a few heuristics\n",
    "# to filter out useless terms early on: the posts are stripped of headers,\n",
    "# footers and quoted replies, and common English words, words occurring in\n",
    "# only one document or in at least 95% of the documents are removed.\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "t0 = time()\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1,\n",
    "                             remove=('headers', 'footers', 'quotes'))\n",
    "data_samples = dataset.data[:n_samples]\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Use tf-idf features for NMF.\n",
    "print(\"Extracting tf-idf features for NMF...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "                                   max_features=n_features,\n",
    "                                   stop_words='english')\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english')\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model with tf-idf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_topics, random_state=1,\n",
    "          alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in NMF model:\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)\n",
    "\n",
    "print(\"Fitting LDA models with tf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "t0 = time()\n",
    "lda.fit(tf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unseen_document = [\"How often to evaluate perplexity. Only used in fit method. set it to 0 or negative number to not evalute perplexity in training at all. Evaluating perplexity can help you check convergence in training process, but it will also increase total training time. Evaluating perplexity in every iteration might increase training time up to two-fold.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tv2 = TfidfVectorizer(vocabulary=tfidf_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unseen_document_tfidf = tv2.fit_transform([unseen_document])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01576719,  0.01576723,  0.01576717,  0.01576696,  0.01576716,\n",
       "         0.01576937,  0.01576692,  0.01576715,  0.01576731,  0.85809354]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.fit_transform(unseen_document_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 100)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset2 = fetch_20newsgroups(shuffle=True, random_state=1,\n",
    "                             remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_samples_2 = dataset2.data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.02612796  0.02612841  0.02612937  0.02612825  0.76484196  0.02612865\n",
      "   0.02612902  0.02612822  0.02612913  0.02612903]]\n",
      "[[ 0.          0.18420905  0.          0.          0.0087455   0.\n",
      "   0.09262394  0.22387758  0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "unseen_document = data_samples[0]\n",
    "tv2 = TfidfVectorizer(vocabulary=tfidf_vectorizer.vocabulary_)\n",
    "unseen_document_tfidf = tv2.fit_transform([unseen_document])\n",
    "print(lda.transform(unseen_document_tfidf))\n",
    "print(nmf.transform(unseen_document_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Well i'm not sure about the story nad it did seem biased. What\\nI disagree with is your statement that the U.S. Media is out to\\nruin Israels reputation. That is rediculous. The U.S. media is\\nthe most pro-israeli media in the world. Having lived in Europe\\nI realize that incidences such as the one described in the\\nletter have occured. The U.S. media as a whole seem to try to\\nignore them. The U.S. is subsidizing Israels existance and the\\nEuropeans are not (at least not to the same degree). So I think\\nthat might be a reason they report more clearly on the\\natrocities.\\n\\tWhat is a shame is that in Austria, daily reports of\\nthe inhuman acts commited by Israeli soldiers and the blessing\\nreceived from the Government makes some of the Holocaust guilt\\ngo away. After all, look how the Jews are treating other races\\nwhen they got power. It is unfortunate.\\n\""
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unseen_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tf_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topic = lda.components_[6,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make win know real drive contact think info did problem sure phone really new windows 24 like format things stuff\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join([tf_feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
